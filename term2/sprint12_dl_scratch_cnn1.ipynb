{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qh_pLx1EUT9-"
   },
   "source": [
    "下ごしらえ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zN6LuTSUT9_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "# ライブラリまでのディレクトリ定義\n",
    "sys.path.append('../ml-scratch/utils') \n",
    "#sys.path.append('../') # colaboratory用\n",
    "\n",
    "import fc, get_mini_batch, relu, soft_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYxQSUsdUT-G"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# 前処理\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# 学習データをスプリット\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "V7QjsGnBfk22",
    "outputId": "c9568e52-2230-4c13-d1dc-b15bc9c3841a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-b0LmYXVGs7"
   },
   "source": [
    "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAZ83L9yVI87"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    チャンネル数を1に限定した1次元畳み込み層クラス\n",
    "    Parameters\n",
    "    ----------\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    stride : int\n",
    "      ストライド\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, W, B, optimizer, stride=1):\n",
    "        self.W = W\n",
    "        self.B = B\n",
    "        self.stride = stride\n",
    "        self.optimizer = optimizer\n",
    "        self.X = None\n",
    "        self.A = None\n",
    "        self.dW = np.zeros(len(W))\n",
    "        self.dX = None\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X = X.copy()                                  # backwardで使うから\n",
    "        A = np.array([])                                   # 返却用\n",
    "        F_idx = np.arange(len(self.W))          # フィルタのindex\n",
    "        \n",
    "        # フィルタ終点がX終点にたどり着くまでやる\n",
    "        while F_idx[-1] <= len(self.X) - 1:\n",
    "            A = np.append(A, X[F_idx] @ self.W + self.B)\n",
    "            # フィルタをストライド分移動\n",
    "            F_idx += self.stride\n",
    "            \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, y, A):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            正解値\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.dX = np.zeros(len(self.X))         # 初期化\n",
    "        self.A = y - A                                       # パラメータ更新で使うためインスタンス変数化\n",
    "        \n",
    "        d_idx = np.arange(len(self.W))     # deltaX計算用index\n",
    "        \n",
    "        # deltaA分やる\n",
    "        for s in range(len(self.A)):\n",
    "            self.dX[d_idx] += self.A[s] * self.W\n",
    "            d_idx += 1    # 右方向\n",
    "                    \n",
    "        # self.W self.Bの更新\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return self.dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        i_s_idx = np.arange(len(layer.W))     # deltaX計算用index\n",
    "        \n",
    "        # deltaA分やる\n",
    "        for s in range(len(layer.A)):\n",
    "            layer.dW += layer.A[s] * layer.X[i_s_idx]\n",
    "            i_s_idx += 1     # 右方向\n",
    "            \n",
    "        layer.W -= self.lr * layer.dW\n",
    "        \n",
    "        layer.dB = np.sum(layer.A, axis=0)\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】1次元畳み込み後の出力サイズの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5KjrBoakVdrP"
   },
   "outputs": [],
   "source": [
    "def calc_output_size(n_in_size, pd_size, st_size, f_size):\n",
    "    return int(1 + (n_in_size + (2 * pd_size) - f_size / st_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1yVQhur4VuXr"
   },
   "source": [
    "## 【問題3】小さな配列での1次元畳み込み層の実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qh504Zi6Vw-S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A : [35. 50.]\n",
      "dW : [ 50.  80. 110.]\n",
      "dB : 30.0\n",
      "dX : [ 30. 110. 170. 140.]\n",
      "outputsize : 2\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3,4]).astype(np.float)    # float型じゃないとブロードキャストできない\n",
    "w = np.array([3, 5, 7]).astype(np.float)     # めんどくさいのでクラスで型変換は後回し\n",
    "b = 1\n",
    "\n",
    "sc1 = SimpleConv1d(w, b, SGD(0.1))\n",
    "A = sc1.forward(x)\n",
    "print('A : %s' % A)\n",
    "\n",
    "# 誤差\n",
    "y = np.array([45, 70])\n",
    "\n",
    "dX = sc1.backward(y, A)\n",
    "dW = sc1.dW\n",
    "dB = sc1.dB\n",
    "print('dW : %s' % dW)\n",
    "print('dB : %s' % dB)\n",
    "print('dX : %s' % dX)\n",
    "print('outputsize : %s' % calc_output_size(len(sc1.X), 0, sc1.stride, len(sc1.W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A : [35. 50. 72.]\n",
      "dW : [ -76.  -88. -142.]\n",
      "dB : -12.0\n",
      "dX : [  30.  110.   44.  -70. -294.]\n",
      "outputsize : 3\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3,4,6]).astype(np.float)\n",
    "w = np.array([3, 5, 7]).astype(np.float)\n",
    "b = 1\n",
    "\n",
    "sc1 = SimpleConv1d(w, b, SGD(0.1))\n",
    "A = sc1.forward(x)\n",
    "print('A : %s' % A)\n",
    "\n",
    "# 誤差\n",
    "y = np.array([45, 70, 30])\n",
    "\n",
    "dX = sc1.backward(y, A)\n",
    "dW = sc1.dW\n",
    "dB = sc1.dB\n",
    "print('dW : %s' % dW)\n",
    "print('dB : %s' % dB)\n",
    "print('dX : %s' % dX)\n",
    "print('outputsize : %s' % calc_output_size(len(sc1.X), 0, sc1.stride, len(sc1.W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7tLxE8mWMfQ"
   },
   "source": [
    "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X1-Sl13mWRud"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    チャンネル数を限定しない1次元畳み込み層クラス\n",
    "    Parameters\n",
    "    ----------\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    stride : int\n",
    "      ストライド\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, W, B, optimizer, stride=1):\n",
    "        self.W = W\n",
    "        self.B = B\n",
    "        self.stride = stride\n",
    "        self.optimizer = optimizer\n",
    "        self.sF = None\n",
    "        self.X = None\n",
    "        self.A = None\n",
    "        self.dW = np.zeros(len(W))\n",
    "        self.dX = None\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X = X\n",
    "        F = len(self.W)          # フィルタサイズ\n",
    "        self.sF = F -1             # フィルタ-1\n",
    "        A = np.empty(self.sF)     # 返却用\n",
    "        \n",
    "        for i in range(self.sF):\n",
    "            i_s = F + i               # i + s\n",
    "            A[i] = X[i : i_s] @ self.W + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, y, A):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            正解値\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初期化\n",
    "        self.dX = np.zeros(len(self.X))\n",
    "        \n",
    "        # パラメータ更新で使うためインスタンス変数化\n",
    "        self.A = y - A\n",
    "        \n",
    "        # self.W self.Bの更新\n",
    "        self = self.optimizer.update(self)\n",
    "              \n",
    "         # TODO:めっちゃイケてないその２\n",
    "        for s in range(len(self.A)):\n",
    "            for i in range(len(self.X)-1):\n",
    "                i_s = i + s\n",
    "                self.dX[i_s] += self.A[s] * self.W[i]\n",
    "        \n",
    "        return self.dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ca3XL2XUT-J"
   },
   "source": [
    "## 【問題5】学習・推定\n",
    "## CNN分類器クラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGQ-6OvQUT-J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging                                                                     # ログ\n",
    "from datetime import datetime                                        # 時間のやつ\n",
    "from sklearn.preprocessing import OneHotEncoder       # ワンホットのやつ\n",
    "from tqdm import tqdm                                                     # 進捗バーを出してくれるやつ\n",
    "\n",
    "\n",
    "class Scratch1dCNNClassifier():\n",
    "    \"\"\"\n",
    "    ニューラルネットワーク分類器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size=10, n_epochs=20,  n_nodes=400, layer=3, verbose=True,\n",
    "                            sigma=1e-2, lr=1e-2, activation=None, optimizer='sgd'):\n",
    "\n",
    "        self.batch_size = batch_size     # バッチサイズ\n",
    "        self.n_epochs = n_epochs         # エポック数 \n",
    "        self.n_input = n_nodes              # 初回のノード数\n",
    "        self.layer = layer                          # 層の数\n",
    "        self.verbose = verbose               # 学習過程出力フラグ\n",
    "        self.activation = activation        #活性化関数(文字列)\n",
    "        self.FCs = []                                  # FCインスタンス格納用\n",
    "        self.activations = []                     # 活性化関数インスタンス格納用\n",
    "        self.loss_ = []                              # 学習用データの学習過程格納用\n",
    "        self.loss_val_ = []                       # 検証用データの学習過程格納用\n",
    "        \n",
    "        # 初期化・最適化クラスインスタンス作成\n",
    "        if activation == 'relu':\n",
    "            self.initializer = HeInitializer()\n",
    "        else:\n",
    "            self.initializer = XavierInitializer()\n",
    "        \n",
    "        if optimizer == 'sgd':\n",
    "            self.optimizer = SGD(lr)\n",
    "        elif optimizer == 'adagrad':\n",
    "            self.optimizer = AdaGrad(lr)\n",
    "            \n",
    "        # ワンホットライブラリのインスタンス作成\n",
    "        self.enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        \n",
    "        # ログレベルを DEBUG に変更\n",
    "        time_stamp = datetime.now().strftime('%Y%m%d')\n",
    "        logging.basicConfig(filename='../tmp/sprint11_' + time_stamp + '.log', level=logging.DEBUG)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        # ワンホット化\n",
    "        y = self.enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        # 検証用データがある場合\n",
    "        if X_val is not None:\n",
    "            y_val= self.enc.fit_transform(y_val[:, np.newaxis])\n",
    "\n",
    "\n",
    "        # 学習用データから特徴量とクラス数を取得\n",
    "        n_features = X.shape[1]\n",
    "        n_output = y.shape[1]\n",
    "\n",
    "        # 初期化\n",
    "        n_nodes1 = self.n_input\n",
    "        \n",
    "        # 各層のFCインスタンス作成\n",
    "        for i in range(self.layer):\n",
    "            if i == (self.layer - 1):     # 出力層\n",
    "                fc = FC(n_nodes2, n_output, self.initializer, self.optimizer)\n",
    "                activation = Softmax()\n",
    "            else:\n",
    "                if i == 0:                       # 入力層\n",
    "                    fc = FC(n_features, n_nodes1, self.initializer, self.optimizer)\n",
    "                else:\n",
    "                    n_nodes2 = int(n_nodes1 / 2)   # TODO: 多分よロしくない\n",
    "                    fc = FC(n_nodes1, n_nodes2, self.initializer, self.optimizer)\n",
    "                    n_nodes1 = n_nodes2\n",
    "                \n",
    "                # 出力層以外は指定された活性化関数をインスタンス化\n",
    "                if self.activation == 'sigmoid':\n",
    "                    activation = Sigmoid()\n",
    "                elif self.activation == 'tanh':\n",
    "                    activation = Tanh()\n",
    "                elif self.activation == 'relu':\n",
    "                    activation = ReLU()\n",
    "                \n",
    "            self.FCs.append(fc)                           # 各自格納\n",
    "            self.activations.append(activation)\n",
    "\n",
    "\n",
    "        # エポックごとに進捗率を計測\n",
    "        for e in tqdm(range(self.n_epochs)):\n",
    "            # ミニバッチ化\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            # ロス格納用\n",
    "            loss_ary = []\n",
    "            \n",
    "            # Xのn_samples / batch_size数分ループ処理\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                                            \n",
    "                # forward propagation\n",
    "                for i in range(self.layer):\n",
    "                    if i == 0:                              # 入力層\n",
    "                        A = self.FCs[i].forward(mini_X_train)\n",
    "                        Z = self.activations[i].forward(A)\n",
    "                    else:\n",
    "                        A = self.FCs[i].forward(Z)\n",
    "                        Z = self.activations[i].forward(A)\n",
    "                \n",
    "                # back propagation\n",
    "                for i in range(self.layer):\n",
    "                    n_FC = self.layer - i - 1      # インスタンス逆指定用\n",
    "                    if i == 0:                               # 入力層\n",
    "                        dA, loss = self.activations[n_FC].backward(Z, mini_y_train)\n",
    "                        loss_ary.append(loss)\n",
    "                    else:                                     # 出力層\n",
    "                        dA = self.activations[n_FC].backward(dZ)\n",
    "                        \n",
    "                    dZ = self.FCs[n_FC].backward(dA)\n",
    "\n",
    "                    \n",
    "            #誤差を格納\n",
    "            self.loss_.append(np.mean(loss_ary))\n",
    "                        \n",
    "            # 検証用データがある場合\n",
    "            if X_val is not None:\n",
    "                # forward propagation\n",
    "                for i in range(self.layer):\n",
    "                    if i == 0:                              # 入力層\n",
    "                        A = self.FCs[i].forward(X_val)\n",
    "                        Z = self.activations[i].forward(A)\n",
    "                    else:\n",
    "                        A = self.FCs[i].forward(Z)\n",
    "                        Z = self.activations[i].forward(A)\n",
    "                \n",
    "                # \n",
    "                dA, loss_val = self.activations[self.layer-1].backward(Z, y_val)\n",
    "\n",
    "                #誤差を格納\n",
    "                self.loss_val_.append(np.mean(loss_val))\n",
    "                            \n",
    "\n",
    "            # フラグがTrueであればログ出力\n",
    "            if self.verbose:\n",
    "                logging.info('forward propagation %sエポック目 sum: %s shape: %s', e+1, np.sum(A), A.shape)\n",
    "                logging.info('forward propagation %sエポック目 sum: %s shape: %s', e+1, np.sum(Z), Z.shape)\n",
    "                logging.info('backward propagation %sエポック目 sum: %s shape: %s', e+1, np.sum(dA), dA.shape)\n",
    "                logging.info('backward propagation %sエポック目 sum: %s shape: %s', e+1, np.sum(dZ), dZ.shape)\n",
    "                logging.info('loss %sエポック目 : %s', e+1, np.sum(loss))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習データ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred :  次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        \n",
    "        # forward propagation\n",
    "        for i in range(self.layer):\n",
    "            if i == 0:                              # 入力層\n",
    "                A = self.FCs[i].forward(X)\n",
    "                Z = self.activations[i].forward(A)\n",
    "            else:\n",
    "                A = self.FCs[i].forward(Z)\n",
    "                Z = self.activations[i].forward(A)\n",
    "        \n",
    "        # 一番確率が高いラベルを予測値に\n",
    "        y_pred = np.argmax(Z, axis=1)\n",
    "        \n",
    "        return y_pred\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sprint12_dl_scratch_cnn1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
