{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sprint22-nlp-intro.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRLDqi12EvKS",
        "colab_type": "text"
      },
      "source": [
        "## 【問題1】BoWとN-gram(手計算)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mvDr4EWGU3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import re\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D\n",
        "from keras.layers import Dense, Input, Dropout, Activation, Flatten, LSTM, GRU, Conv1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.feature_selection import SelectKBest,f_classif\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL3-KSOkEFyx",
        "colab_type": "code",
        "outputId": "3a149708-ce61-47fb-9f1f-915c7a6f1c52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "vocabulary = [\"今\", \"撮影\", \"中\", \"です\", \"休憩\", \"今日\", \"ドラマ\", \"映画\", \"瞬\", \"公開\"]\n",
        "ms_kk_texts = [\"今撮影中です\",\n",
        "                              \"今休憩中です\", \n",
        "                              \"今日ドラマ撮影です\", \n",
        "                              \"今日映画瞬公開です\"]\n",
        "texts_vec = [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0], \n",
        "                        [1, 0, 1, 1, 1, 0, 0, 0, 0, 0], \n",
        "                        [0, 1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
        "                        [0, 0, 0, 1, 0, 1, 0, 1, 1, 1]]\n",
        "\n",
        "df_bow_1gram = pd.DataFrame(data = texts_vec, columns = vocabulary, index = ms_kk_texts)\n",
        "df_bow_1gram"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>今</th>\n",
              "      <th>撮影</th>\n",
              "      <th>中</th>\n",
              "      <th>です</th>\n",
              "      <th>休憩</th>\n",
              "      <th>今日</th>\n",
              "      <th>ドラマ</th>\n",
              "      <th>映画</th>\n",
              "      <th>瞬</th>\n",
              "      <th>公開</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>今撮影中です</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>今休憩中です</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>今日ドラマ撮影です</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>今日映画瞬公開です</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           今  撮影  中  です  休憩  今日  ドラマ  映画  瞬  公開\n",
              "今撮影中です     1   1  1   1   0   0    0   0  0   0\n",
              "今休憩中です     1   0  1   1   1   0    0   0  0   0\n",
              "今日ドラマ撮影です  0   1  0   1   0   1    1   0  0   0\n",
              "今日映画瞬公開です  0   0  0   1   0   1    0   1  1   1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E99EPuzbFaKx",
        "colab_type": "text"
      },
      "source": [
        "## 【問題2】TF-IDF(手計算)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CEG_5L8FcLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_term_frequency(word, text, vec):\n",
        "    word_frequency_count = text.count(word)\n",
        "    n_word_in_text = sum(vec)\n",
        "\n",
        "    return word_frequency_count / n_word_in_text\n",
        "\n",
        "\n",
        "def calc_inverse_document_frequency(word, texts, vecs, w_index):\n",
        "    text_count = len(texts)\n",
        "    text_count_included_word = sum([vec[w_index] for vec in vecs])\n",
        "\n",
        "    return math.log2(text_count / text_count_included_word)\n",
        "\n",
        "\n",
        "def calc_tf_idf(words, texts, vecs, w_index, t_index):  \n",
        "    tf = calc_term_frequency(words[w_index], texts[t_index], vecs[t_index])\n",
        "    idf = calc_inverse_document_frequency(words[w_index], texts, vecs, w_index)\n",
        "                                  \n",
        "    return tf * idf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjeLtd1c3J2S",
        "colab_type": "code",
        "outputId": "3c0742a3-504d-47ad-ed24-eea1063683c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# \"今\"のインデックスを取得\n",
        "w_index = vocabulary.index(\"今\")\n",
        "# \"今撮影中です\"のインデックスを取得\n",
        "t_index = ms_kk_texts.index(\"今撮影中です\")\n",
        "\n",
        "print(calc_tf_idf(vocabulary, ms_kk_texts, texts_vec, w_index, t_index))\n",
        "\n",
        "# \"です\"のインデックスを取得\n",
        "w_index = vocabulary.index(\"です\")\n",
        "# \"今日ドラマ撮影です\"のインデックスを取得\n",
        "t_index = ms_kk_texts.index(\"今日ドラマ撮影です\")\n",
        "\n",
        "print(calc_tf_idf(vocabulary, ms_kk_texts, texts_vec, w_index, t_index))\n",
        "\n",
        "# \"瞬\"のインデックスを取得\n",
        "w_index = vocabulary.index(\"瞬\")\n",
        "# \"今日映画瞬公開です\t\"のインデックスを取得\n",
        "t_index = ms_kk_texts.index(\"今日映画瞬公開です\")\n",
        "\n",
        "print(calc_tf_idf(vocabulary, ms_kk_texts, texts_vec, w_index, t_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.25\n",
            "0.0\n",
            "0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mMjrp3AFckm",
        "colab_type": "text"
      },
      "source": [
        "## 【問題3】テキストクリーニング(プログラミング)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0WU-7M4FeX5",
        "colab_type": "code",
        "outputId": "fdfdd8d4-238f-4772-a235-3b7e82a572ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 対象テキストデータ\n",
        "text = \"<!everyone> *【スペシャル特典】有償のRubyMineやPyCharmの `6ヶ月間100%OFFクーポン` をご希望者の方先着100名様に贈呈いたします！*\\n\\nこの度、RubyMineやPyCharmのメーカーであるJetBrains社へのクーポンコードの提供交渉が実り、100クーポンをいただくことができました。\\n\\n```\\nRubyMine\\n<https://www.jetbrains.com/ruby/>\\n\\nPyCharm\\n<https://www.jetbrains.com/pycharm/>\\n```\\n\\n「ご希望の方は、手を挙げて！」方式で、ご希望の方はこの投稿の手あげスタンプをクリックしてください。\\n\\n期限は、 *`2019年3月20日（水）22:00まで`* とさせていただきます。\\nふるってのご希望をお待ちしております！ :smile:\"\n",
        "\n",
        "# 正規表現一覧\n",
        "symbol_reg = r'[\\n*` ]+'\n",
        "mention_reg = r'<.*?>'\n",
        "phraze_reg = r'【.*?】'\n",
        "command_reg = r':[^0-9０-９]+:'\n",
        "\n",
        "reg_str = f'{symbol_reg}|{mention_reg}|{phraze_reg}|{command_reg}'\n",
        "reg_str = re.compile(reg_str)\n",
        "hoge = re.sub(reg_str, '', text)\n",
        "# re.sub(r'[\\n*！`]+', '', text)でもできます\n",
        "print(hoge)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "有償のRubyMineやPyCharmの6ヶ月間100%OFFクーポンをご希望者の方先着100名様に贈呈いたします！この度、RubyMineやPyCharmのメーカーであるJetBrains社へのクーポンコードの提供交渉が実り、100クーポンをいただくことができました。RubyMinePyCharm「ご希望の方は、手を挙げて！」方式で、ご希望の方はこの投稿の手あげスタンプをクリックしてください。期限は、2019年3月20日（水）22:00までとさせていただきます。ふるってのご希望をお待ちしております！\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPp8oFjCFfJm",
        "colab_type": "text"
      },
      "source": [
        "## 【問題4】形態素解析\n",
        "janomeをcolab上にインストール・インポート  \n",
        "使い方：https://note.nkmk.me/python-janome-tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3Cjc-trFi9S",
        "colab_type": "code",
        "outputId": "f1173062-dff5-4bca-9daa-b0d051cab095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install janome"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/7c/560f4c9ff01a584b1ecd1da981e82d0077c079ecba84571b4f623680300e/Janome-0.3.9-py2.py3-none-any.whl (25.1MB)\n",
            "\u001b[K     |████████████████████████████████| 25.1MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.3.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loR366ZRp2qm",
        "colab_type": "code",
        "outputId": "63efd0f3-f93c-471a-f28a-b72511492203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "t = Tokenizer()\n",
        "for token in t.tokenize(hoge):\n",
        "    print(token)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "有償\t名詞,一般,*,*,*,*,有償,ユウショウ,ユーショー\n",
            "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
            "RubyMine\t名詞,一般,*,*,*,*,RubyMine,*,*\n",
            "や\t助詞,並立助詞,*,*,*,*,や,ヤ,ヤ\n",
            "PyCharm\t名詞,一般,*,*,*,*,PyCharm,*,*\n",
            "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
            "6\t名詞,数,*,*,*,*,6,*,*\n",
            "ヶ月\t名詞,接尾,助数詞,*,*,*,ヶ月,カゲツ,カゲツ\n",
            "間\t名詞,接尾,一般,*,*,*,間,カン,カン\n",
            "100\t名詞,数,*,*,*,*,100,*,*\n",
            "%\t名詞,サ変接続,*,*,*,*,%,*,*\n",
            "OFF\t名詞,一般,*,*,*,*,OFF,*,*\n",
            "クーポン\t名詞,一般,*,*,*,*,クーポン,クーポン,クーポン\n",
            "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
            "ご\t接頭詞,名詞接続,*,*,*,*,ご,ゴ,ゴ\n",
            "希望\t名詞,サ変接続,*,*,*,*,希望,キボウ,キボー\n",
            "者\t名詞,接尾,一般,*,*,*,者,シャ,シャ\n",
            "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
            "方\t名詞,非自立,一般,*,*,*,方,ホウ,ホー\n",
            "先着\t名詞,サ変接続,*,*,*,*,先着,センチャク,センチャク\n",
            "100\t名詞,数,*,*,*,*,100,*,*\n",
            "名\t名詞,接尾,助数詞,*,*,*,名,メイ,メイ\n",
            "様\t名詞,接尾,人名,*,*,*,様,サマ,サマ\n",
            "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
            "贈呈\t名詞,サ変接続,*,*,*,*,贈呈,ゾウテイ,ゾーテイ\n",
            "いたし\t動詞,非自立,*,*,五段・サ行,連用形,いたす,イタシ,イタシ\n",
            "ます\t助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス\n",
            "！\t記号,一般,*,*,*,*,！,！,！\n",
            "この\t連体詞,*,*,*,*,*,この,コノ,コノ\n",
            "度\t名詞,非自立,副詞可能,*,*,*,度,タビ,タビ\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "RubyMine\t名詞,固有名詞,組織,*,*,*,RubyMine,*,*\n",
            "や\t助詞,並立助詞,*,*,*,*,や,ヤ,ヤ\n",
            "PyCharm\t名詞,一般,*,*,*,*,PyCharm,*,*\n",
            "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
            "メーカー\t名詞,一般,*,*,*,*,メーカー,メーカー,メーカー\n",
            "で\t助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\n",
            "ある\t助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\n",
            "JetBrains\t名詞,一般,*,*,*,*,JetBrains,*,*\n",
            "社\t名詞,接尾,一般,*,*,*,社,シャ,シャ\n",
            "へ\t助詞,格助詞,一般,*,*,*,へ,ヘ,エ\n",
            "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
            "クーポン\t名詞,一般,*,*,*,*,クーポン,クーポン,クーポン\n",
            "コード\t名詞,一般,*,*,*,*,コード,コード,コード\n",
            "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
            "提供\t名詞,サ変接続,*,*,*,*,提供,テイキョウ,テイキョー\n",
            "交渉\t名詞,サ変接続,*,*,*,*,交渉,コウショウ,コーショー\n",
            "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
            "実り\t動詞,自立,*,*,五段・ラ行,連用形,実る,ミノリ,ミノリ\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "100\t名詞,数,*,*,*,*,100,*,*\n",
            "クーポン\t名詞,一般,*,*,*,*,クーポン,クーポン,クーポン\n",
            "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
            "いただく\t動詞,自立,*,*,五段・カ行イ音便,基本形,いただく,イタダク,イタダク\n",
            "こと\t名詞,非自立,一般,*,*,*,こと,コト,コト\n",
            "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
            "でき\t動詞,自立,*,*,一段,連用形,できる,デキ,デキ\n",
            "まし\t助動詞,*,*,*,特殊・マス,連用形,ます,マシ,マシ\n",
            "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
            "。\t記号,句点,*,*,*,*,。,。,。\n",
            "RubyMinePyCharm\t名詞,一般,*,*,*,*,RubyMinePyCharm,*,*\n",
            "「\t記号,括弧開,*,*,*,*,「,「,「\n",
            "ご\t接頭詞,名詞接続,*,*,*,*,ご,ゴ,ゴ\n",
            "希望\t名詞,サ変接続,*,*,*,*,希望,キボウ,キボー\n",
            "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
            "方\t名詞,非自立,一般,*,*,*,方,ホウ,ホー\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "手\t名詞,一般,*,*,*,*,手,テ,テ\n",
            "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
            "挙げ\t動詞,自立,*,*,一段,連用形,挙げる,アゲ,アゲ\n",
            "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
            "！\t記号,一般,*,*,*,*,！,！,！\n",
            "」\t記号,括弧閉,*,*,*,*,」,」,」\n",
            "方式\t名詞,一般,*,*,*,*,方式,ホウシキ,ホーシキ\n",
            "で\t助詞,格助詞,一般,*,*,*,で,デ,デ\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "ご\t接頭詞,名詞接続,*,*,*,*,ご,ゴ,ゴ\n",
            "希望\t名詞,サ変接続,*,*,*,*,希望,キボウ,キボー\n",
            "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
            "方\t名詞,非自立,一般,*,*,*,方,ホウ,ホー\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "この\t連体詞,*,*,*,*,*,この,コノ,コノ\n",
            "投稿\t名詞,サ変接続,*,*,*,*,投稿,トウコウ,トーコー\n",
            "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
            "手\t名詞,一般,*,*,*,*,手,テ,テ\n",
            "あげ\t動詞,自立,*,*,一段,連用形,あげる,アゲ,アゲ\n",
            "スタンプ\t名詞,一般,*,*,*,*,スタンプ,スタンプ,スタンプ\n",
            "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
            "クリック\t名詞,一般,*,*,*,*,クリック,クリック,クリック\n",
            "し\t動詞,自立,*,*,サ変・スル,連用形,する,シ,シ\n",
            "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
            "ください\t動詞,非自立,*,*,五段・ラ行特殊,命令ｉ,くださる,クダサイ,クダサイ\n",
            "。\t記号,句点,*,*,*,*,。,。,。\n",
            "期限\t名詞,一般,*,*,*,*,期限,キゲン,キゲン\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "2019\t名詞,数,*,*,*,*,2019,*,*\n",
            "年\t名詞,接尾,助数詞,*,*,*,年,ネン,ネン\n",
            "3\t名詞,数,*,*,*,*,3,*,*\n",
            "月\t名詞,一般,*,*,*,*,月,ツキ,ツキ\n",
            "20\t名詞,数,*,*,*,*,20,*,*\n",
            "日\t名詞,接尾,助数詞,*,*,*,日,ニチ,ニチ\n",
            "（\t記号,括弧開,*,*,*,*,（,（,（\n",
            "水\t名詞,一般,*,*,*,*,水,ミズ,ミズ\n",
            "）\t記号,括弧閉,*,*,*,*,）,）,）\n",
            "22\t名詞,数,*,*,*,*,22,*,*\n",
            ":\t名詞,サ変接続,*,*,*,*,:,*,*\n",
            "00\t名詞,数,*,*,*,*,00,*,*\n",
            "まで\t助詞,副助詞,*,*,*,*,まで,マデ,マデ\n",
            "と\t助詞,格助詞,引用,*,*,*,と,ト,ト\n",
            "さ\t動詞,自立,*,*,サ変・スル,未然レル接続,する,サ,サ\n",
            "せ\t動詞,接尾,*,*,一段,連用形,せる,セ,セ\n",
            "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
            "いただき\t動詞,非自立,*,*,五段・カ行イ音便,連用形,いただく,イタダキ,イタダキ\n",
            "ます\t助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス\n",
            "。\t記号,句点,*,*,*,*,。,。,。\n",
            "ふるって\t副詞,一般,*,*,*,*,ふるって,フルッテ,フルッテ\n",
            "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
            "ご\t接頭詞,名詞接続,*,*,*,*,ご,ゴ,ゴ\n",
            "希望\t名詞,サ変接続,*,*,*,*,希望,キボウ,キボー\n",
            "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
            "お待ち\t名詞,サ変接続,*,*,*,*,お待ち,オマチ,オマチ\n",
            "し\t動詞,自立,*,*,サ変・スル,連用形,する,シ,シ\n",
            "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
            "おり\t動詞,非自立,*,*,五段・ラ行,連用形,おる,オリ,オリ\n",
            "ます\t助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス\n",
            "！\t記号,一般,*,*,*,*,！,！,！\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mej0TapariT8",
        "colab_type": "code",
        "outputId": "4e04fd51-fb78-4506-f45c-f98519e9306c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print([token.surface for token in t.tokenize(hoge)\n",
        "       if token.part_of_speech.split(',')[0] in ['名詞', '動詞']])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['有償', 'RubyMine', 'PyCharm', '6', 'ヶ月', '間', '100', '%', 'OFF', 'クーポン', '希望', '者', '方', '先着', '100', '名', '様', '贈呈', 'いたし', '度', 'RubyMine', 'PyCharm', 'メーカー', 'JetBrains', '社', 'クーポン', 'コード', '提供', '交渉', '実り', '100', 'クーポン', 'いただく', 'こと', 'でき', 'RubyMinePyCharm', '希望', '方', '手', '挙げ', '方式', '希望', '方', '投稿', '手', 'あげ', 'スタンプ', 'クリック', 'し', 'ください', '期限', '2019', '年', '3', '月', '20', '日', '水', '22', ':', '00', 'さ', 'せ', 'いただき', '希望', 'お待ち', 'し', 'おり']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IonDjOHNFjfP",
        "colab_type": "text"
      },
      "source": [
        "## 【問題5】ニュースの分析"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sCUx6E4FlH_",
        "colab_type": "code",
        "outputId": "b75bcdf4-2c7e-44ce-bf93-9eff69884833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# livedoorのnewsをダウンロード\n",
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "# 圧縮ファイルを解凍\n",
        "!tar zxf ldcc-20140209.tar.gz\n",
        "# livedoorニュースの説明を表示\n",
        "#!cat text/README.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-01 01:36:16--  https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
            "Resolving www.rondhuit.com (www.rondhuit.com)... 59.106.19.174\n",
            "Connecting to www.rondhuit.com (www.rondhuit.com)|59.106.19.174|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8855190 (8.4M) [application/x-gzip]\n",
            "Saving to: ‘ldcc-20140209.tar.gz’\n",
            "\n",
            "ldcc-20140209.tar.g 100%[===================>]   8.44M  2.68MB/s    in 3.6s    \n",
            "\n",
            "2019-07-01 01:36:21 (2.35 MB/s) - ‘ldcc-20140209.tar.gz’ saved [8855190/8855190]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9bNaf6gzCcw",
        "colab_type": "text"
      },
      "source": [
        " - まずどんなニュースなのか読んでみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcW3txzTuox9",
        "colab_type": "code",
        "outputId": "6dc0154d-1390-4a40-ad0d-17b420ed7596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# encodingをutf-8指定して読み込み\n",
        "bin_data = load_files('./text', encoding='utf-8')\n",
        "documents = bin_data.data\n",
        "# 今回はラベルが無いと仮定してください\n",
        "# targets = bin_data.target\n",
        "documents[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'http://news.livedoor.com/article/detail/4931238/\\n2010-08-08T10:00:00+0900\\nNY名物イベントが日本でも！名店グルメを気軽に楽しむ\\nニューヨークで20年続いている食の祭典「レストラン・ウィーク」。その日本版がダイナーズクラブ特別協賛のもと7月30日よりスタート。8月31日までの期間中、青山・六本木、丸の内、銀座、横浜のエリアから、ラグジュアリーレストラン81店舗がこのイベントのために特別用意したランチメニュー2010円（税・サ別）、ディナー5000円（税・サ別）を気軽に楽しめる、とっておきのイベントです。\\n\\u3000\\n\\u3000実行委員長には、学校法人服部学園、服部栄養専門学校 理事長・校長であり医学博士でもある服部幸應氏を迎え、実行委員に石田純一さん、LA BETTOLAオーナーシェフ落合務氏、フードアナリスト協会会長、高賀右近氏、つきぢ田村三代目、田村隆氏に、そして放送作家・脚本家の小山薫堂さんなど、食のスペシャリストたちが勢揃い。\\n\\n参加レストランには、ミシュランのフランス版、東京版ともに星を獲得している吉野建シェフの「レストラン タテル ヨシノ 汐留」や、日本料理の名門「つきぢ田村」、「金田中 庵」、「赤坂璃宮」に「mikuni MARUNOUCHI」など、日本を代表するレストランがずらり。\\n\\u3000イベント期間の〜8月19日までは、特別協賛のダイナーズクラブカード会員、またはシティバンクに口座を持つシティゴールドメンバーが楽しめる先行期間となりますが、その後は誰でも参加できるので、日程のチェックは必須。\\n\\n\\u3000予約方法は必ず事前に、各店舗に問合せを行い「ジャパンレストラン・ウィーク2010」での予約であることを伝えればOK！憧れていたレストランの料理をリーズナブルにいただけるチャンスです！極上の味とラグジュアリーな空間を満喫。そんな幸せを実感できる「ジャパンレストラン・ウィーク2010」にぜひ参加しててみてはいかがですか？\\n\\nJAPAN RESTAURANT WEEK 2010 -公式サイト\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK19OOrTzFou",
        "colab_type": "text"
      },
      "source": [
        "- 出現単語をカウントして分析する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZK1Is_MzIDU",
        "colab_type": "code",
        "outputId": "b82f759c-1e91-4fb0-cfdf-f28e356b6312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "t = Tokenizer()\n",
        "c = collections.Counter(t.tokenize(documents[0], wakati=True))\n",
        "print(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'、': 28, 'の': 17, 'に': 14, 'を': 11, ' ': 11, '「': 8, '・': 8, '」': 8, 'が': 7, 'て': 7, 'レストラン': 7, '。': 7, 'は': 7, '\\n': 6, 'で': 6, '2010': 5, '/': 4, 'イベント': 4, '日本': 4, '\\u3000': 4, '-': 3, '！': 3, 'ウィーク': 3, '版': 3, '特別': 3, '月': 3, '日': 3, '期間': 3, 'し': 3, 'です': 3, '服部': 3, '氏': 3, '田村': 3, '\\n\\n': 3, '参加': 3, '.': 2, '08': 2, ':': 2, '00': 2, 'も': 2, '気軽': 2, 'いる': 2, '食': 2, '協賛': 2, '8': 2, 'まで': 2, '店舗': 2, 'た': 2, '円': 2, '（': 2, '税': 2, 'サ': 2, '別': 2, '）': 2, '楽しめる': 2, '実行': 2, '委員': 2, '長': 2, '学校': 2, 'ある': 2, 'さん': 2, 'シェフ': 2, 'つき': 2, 'ぢ': 2, 'など': 2, '料理': 2, 'と': 2, 'できる': 2, '予約': 2, 'ジャパン': 2, 'http': 1, '://': 1, 'news': 1, 'livedoor': 1, 'com': 1, 'article': 1, 'detail': 1, '4931238': 1, 'T': 1, '10': 1, '+': 1, '0900': 1, 'NY': 1, '名物': 1, '名店': 1, 'グルメ': 1, '楽しむ': 1, 'ニューヨーク': 1, '20': 1, '年': 1, '続い': 1, '祭典': 1, 'その': 1, 'ダイナーズクラブ': 1, 'もと': 1, '7': 1, '30': 1, 'より': 1, 'スタート': 1, '31': 1, '中': 1, '青山': 1, '六本木': 1, '丸の内': 1, '銀座': 1, '横浜': 1, 'エリア': 1, 'から': 1, 'ラグジュアリーレストラン': 1, '81': 1, 'この': 1, 'ため': 1, '用意': 1, 'ランチ': 1, 'メニュー': 1, 'ディナー': 1, '5000': 1, 'とっ': 1, 'おき': 1, '法人': 1, '学園': 1, '栄養': 1, '専門': 1, '理事': 1, '校長': 1, 'あり': 1, '医学': 1, '博士': 1, '幸': 1, '應氏': 1, '迎え': 1, '石田': 1, '純一': 1, 'LA': 1, 'BETTOLA': 1, 'オーナー': 1, '落合': 1, '務': 1, 'フード': 1, 'アナリスト': 1, '協会': 1, '会長': 1, '高賀': 1, '右近': 1, '三': 1, '代目': 1, '隆': 1, 'そして': 1, '放送': 1, '作家': 1, '脚本': 1, '家': 1, '小山': 1, '薫': 1, '堂': 1, 'スペシャリスト': 1, 'たち': 1, '勢揃い': 1, 'ミシュラン': 1, 'フランス': 1, '東京': 1, 'とも': 1, '星': 1, '獲得': 1, '吉野': 1, '建': 1, 'タテル': 1, 'ヨシノ': 1, '汐留': 1, 'や': 1, '名門': 1, '金': 1, '田中': 1, '庵': 1, '赤坂': 1, '璃宮': 1, 'mikuni': 1, 'MARUNOUCHI': 1, '代表': 1, 'する': 1, 'ずらり': 1, '〜': 1, '19': 1, 'ダイナーズクラブカード': 1, '会員': 1, 'または': 1, 'シティバンク': 1, '口座': 1, '持つ': 1, 'シティゴールドメンバー': 1, '先行': 1, 'なり': 1, 'ます': 1, 'その後': 1, '誰': 1, 'でも': 1, 'ので': 1, '日程': 1, 'チェック': 1, '必須': 1, '方法': 1, '必ず': 1, '事前': 1, '各': 1, '問合せ': 1, '行い': 1, 'こと': 1, '伝えれ': 1, 'ば': 1, 'OK': 1, '憧れ': 1, 'い': 1, 'リーズナブル': 1, 'いただける': 1, 'チャンス': 1, '極上': 1, '味': 1, 'ラグジュアリー': 1, 'な': 1, '空間': 1, '満喫': 1, 'そんな': 1, '幸せ': 1, '実感': 1, 'ぜひ': 1, 'み': 1, 'いかが': 1, 'か': 1, '？': 1, 'JAPAN': 1, 'RESTAURANT': 1, 'WEEK': 1, '公式': 1, 'サイト': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvQO5PMF0Dh_",
        "colab_type": "text"
      },
      "source": [
        "単語で一番多かったのは「レストラン」、ついで「イベント」「日本」  \n",
        "助詞と「、」が多かった。記号が多いのでクリーニングしたほうがよさそう。  \n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "- テキストをクリーニングする"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UTD4Mqm09j9",
        "colab_type": "code",
        "outputId": "f7f5385c-c3f0-49d8-ac92-b402589e80ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 正規表現一覧\n",
        "symbol_reg = r'[\\n*`\\-\\+\\[\\]\\'…\\/「」・（）■“”『』／()−＜＞［］【】◯◆\\- 　]+'\n",
        "time_reg = r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\+\\d{4}'\n",
        "url_reg = r'https?:\\/\\/[\\w\\/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+'\n",
        "\n",
        "reg_str = f'{symbol_reg}|{time_reg}|{url_reg}'\n",
        "reg_str = re.compile(reg_str)\n",
        "\n",
        "cleaned_docs = []\n",
        "\n",
        "# クリーニング\n",
        "for doc in documents:\n",
        "    cleaned_docs.append(re.sub(reg_str, '', doc))\n",
        "\n",
        "print(cleaned_docs[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['NY名物イベントが日本でも！名店グルメを気軽に楽しむニューヨークで20年続いている食の祭典レストランウィーク。その日本版がダイナーズクラブ特別協賛のもと7月30日よりスタート。8月31日までの期間中、青山六本木、丸の内、銀座、横浜のエリアから、ラグジュアリーレストラン81店舗がこのイベントのために特別用意したランチメニュー2010円税サ別、ディナー5000円税サ別を気軽に楽しめる、とっておきのイベントです。実行委員長には、学校法人服部学園、服部栄養専門学校理事長校長であり医学博士でもある服部幸應氏を迎え、実行委員に石田純一さん、LABETTOLAオーナーシェフ落合務氏、フードアナリスト協会会長、高賀右近氏、つきぢ田村三代目、田村隆氏に、そして放送作家脚本家の小山薫堂さんなど、食のスペシャリストたちが勢揃い。参加レストランには、ミシュランのフランス版、東京版ともに星を獲得している吉野建シェフのレストランタテルヨシノ汐留や、日本料理の名門つきぢ田村、金田中庵、赤坂璃宮にmikuniMARUNOUCHIなど、日本を代表するレストランがずらり。イベント期間の〜8月19日までは、特別協賛のダイナーズクラブカード会員、またはシティバンクに口座を持つシティゴールドメンバーが楽しめる先行期間となりますが、その後は誰でも参加できるので、日程のチェックは必須。予約方法は必ず事前に、各店舗に問合せを行いジャパンレストランウィーク2010での予約であることを伝えればOK！憧れていたレストランの料理をリーズナブルにいただけるチャンスです！極上の味とラグジュアリーな空間を満喫。そんな幸せを実感できるジャパンレストランウィーク2010にぜひ参加しててみてはいかがですか？JAPANRESTAURANTWEEK2010公式サイト', '小沢一郎氏の妻が支援者に離婚を報告。週刊文春報じる13日、Web版週刊文春は、民主党の元代表小沢一郎氏の妻が、支援者宛に離婚したことを伝える手紙を送ったと報じ、ツイッターやネット掲示板で大きな話題になっている。記事によると、その手紙は小沢は放射能が怖くて秘書と一緒に逃げだしました隠し子が発覚した際、小沢元代表は和子夫人に謝るどころか、いつでも離婚してやると言い放ち、和子夫人は一時は自殺まで考えたという小沢氏の支持者にとってはショッキングな内容となっている。ツイッターやネット掲示板では、これが小沢一郎の本性かこりゃすごいな。てか、がっかりだなあなどと、手紙の内容に驚く声が相次ぎ、その他にも小沢潰しですかね?元秘書が交番で暴れて逮捕された件といい、小沢氏を潰したい何者かによる工作活動では?といった声が挙がっていた。関連情報小沢一郎夫人が支援者に離婚しました週刊文春', 'SportsWatch田中＆里田の交際、アプローチは里田からグラビアアイドルほしのあき＆騎手三浦皇成がお互いのブログで交際を認めた日、東北楽天ゴールデンイーグルスのエース田中将大とタレント里田まいの交際もまた公のものとなったが、本日30日火発売の週刊アサヒ芸能12.9号では、マー君をナンパした里田との見出しで、両者の交際にまつわる関係者の証言を紹介した。同誌にコメントを寄せた芸能デスクによると、周囲の事情はさておき、当人たちが盛り上がっているのは確か。結婚も完全に視野に入れているようで、すでに新居を探しているとの話まであるという。また、二人の交際は里田からのアプローチによるものとのことで、前出の芸能デスクは、2人は昨年末の番組共演時に、里田から猛アタック。一緒に食事でもと誘い、春頃にはつきあい始めた。里田は仲のいいスザンヌ24、木下優樹菜22には、早くから相談していたし、周囲にも浮かれてしゃべりまくっていた。次世代エースをゲットしたわけですから、賞味期限切れ間近の里田にすれば、してやったりでしょうとも語っている。ちなみに、同じく同誌にコメントする球団関係者は、高校時代は同じ高校の女子生徒と在学中の3年間、ずっとつきあっていましたね。基本はオクテ。プロ入り後にキャバクラなども覚えましたが、まぁ、モテるタイプではないし、女あしらいも不慣れ。高校時代はかなりワガママで、元カノは苦労が絶えなかったと聞いています。同年代の女より、里田のような姉さんタイプのほうが勝負の世界では向いているケースも多いですからねと明かしており、この交際も田中にとっては吉とした。週刊アサヒ芸能ライト版デジタルPC版週刊アサヒ芸能モバイル版', '被災地の缶詰を途上国に正気じゃない。人殺しだ!!30日、政府が被災地で製造された水産加工品を発展途上国へ送ると発表したが、その内容があまりにも酷いとネット掲示板で非難が殺到している。MSN産経ニュースの記事によると、東日本大震災の被災地で製造されたイワシやサバの水産加工品を、カンボジアなどの発展途上国の人々に食べてもらうため、政府は国連機関の世界食糧計画WFPと政府開発援助ODAに関する書簡を交換したことを発表。外務省の幹部は安全性に問題がないものを輸出することで、海外に根強い風評被害の打破を図りたいと述べているが、一部の市民団体は原発事故の影響を懸念して反発しているという。この動きに対してネット掲示板では風評被害の解決を途上国を使って解消とか、正気とは思えない。人殺しだろこんなことしたら日本の信頼を損ねるだろうが餓死寸前の子供にセシウム入りの缶詰食わせるなんて食べさせて応援ってなんか強制みたいだなと政府を非難する声が相次いだ。また、こんなことをしたら風評被害がより拡大するのでは?と懸念する声もあがっており、今後の政府の動きに注目が集まっている。関連情報食料支援で風評被害解消被災地の缶詰を途上国にMSN産経ニュース関連情報食べさせて応援被災地の缶詰を途上国に送りつけて風評被害を解決！livedoorBlog', '仲間由紀恵さん、生徒亀梨和也さんを大人の魅力が出てきたと絶賛11月に入り各地のイルミネーションが点灯されると、グっと高まるクリスマスシーズン。女子の皆さんは彼へのプレゼント、そして彼におねだりするプレゼントの目星はついていますか？男性って何を貰ったら嬉しいんだろう？毎年アクセサリーやバッグじゃマンネリとお悩みの皆さん。今年は美容家電のプレゼントはいかがでしょうか。2日、ザペニンシュラ東京にてPanasonicBeautyキレイを贈るクリスマスキャンペーンの発表会が行われ、イメージキャラクターの仲間由紀恵さんと亀梨和也さんが登場しました。仲間さんはナノケアシリーズ、亀梨さんは、ラムダッシュのCMでお馴染みですが、2人もプライベートで商品を愛用しているそうです。特に、仲間さんは美しい黒髪の秘訣であるナノケアヘアードライヤーをお友達にプレゼントしたことがあるそうです。実は、最近仲間さんの様に美容家電を大切な人にプレゼントする人が増加中。夜寝たままお肌のケアを行える人気のナイトスチーマーをプレゼントされたと回答した人は、3年前には使用者全体の12%でしたが、現在は28%と、4人に1人以上がプレゼントされている事が分かっています。パナソニックが女性約100名に定番以外でプレゼントして欲しいものは？と聞いた調査では、美容家電エステ機器を回答した人が36%とダントツの1位。実用的でありながら、自分を美しくしてくれるアイテムに人気が集まる結果となりました。彼にもらったプレゼントで、彼の為にもっと美しくなりたい！そんな女ゴコロがこめられているのかもしれませんね。PanasonicBeautyキレイを贈るクリスマスキャンペーンでは、本日2日から2010年12月31日金までの期間中、202名に素敵なプレゼントが当たるチャンスが。中でも、仲間さんがプロデュースしたオリジナルデコラムダッシュ、亀梨さんがプロデュースしたオリジナルデコスチーマーは世界に1つしかない、超レアアイテムです。亀梨さんをイメージして、ラムダッシュのデコデザインをしたという仲間さんは大人の魅力が出てきた亀梨さんにピッタリな、豪華で格好良いデザインにしました。5年前、初対面だったドラマの撮影では演技の経験があまり無いんですと初々しい表情を見せていた子が、みるみるうちに大きくなり、先生は嬉しいです！と、ドラマごくせんのキャラクターヤンクミの表情を一瞬のぞかせました。一方の亀梨さんは仲間さんについてきれいなお姉さんだけど、キュートな一面もあって、ドラマの撮影時には生徒役の皆でマジでカワイイよねって話していましたと当時の思い出話を語りながらも、美容家電のプレゼントって、他のプレゼントよりも相手と近くなれるというか、愛情を感じますとコメント。司会者から亀梨さんはこのラムダッシュを誰に贈りたいですか？と尋ねられると聖KATTUNのメンバー田中聖が、新しいの欲しがってるんですよ。でも、贈るかどうかは考えます笑と会場の笑いを誘っていました。本日からオープンしたPanasonicBeautyキレイを贈るクリスマス特設サイトでは、プレゼントキャンペーンの詳細の他、彼や彼女に欲しいアイテムをおねだり出来るおねだりグリーティング機能も。あなたも、LOVEメッセージ付きで、可愛く彼におねだりしちゃいましょう！PanasonicBeautyキレイを贈るクリスマスキャンペーンサイト', 'セクハラや不倫が横行している会社です辛口説教部屋vol.433年で転職は早すぎる？将来が見えない仕事が面白くない若手社会人の悩みは尽きないもの。そんな様々な悩みに辛口4姉妹がお答えします。今回のお悩みQ：セクハラや不倫が横行している会社です。こんな会社でも三年い続けないと評価してもらえないのでしょうか。不動産業営業25歳女性<悩める相談者No.043>現在入社2年目。法人営業をしています。女性を積極的に採用している会社ということで、男女分け隔てなく活躍できると思い今の会社に入社しました。しかし、入社してみたらびっくり。女性は上司のセクハラ対象だったのです。社内恋愛が多い会社とは聞いていて、仲の良い会社なんだな、と思っていましたが、社内不倫まで横行している始末。仕事内容は楽しいのですが、こんな会社にこれ以上いるのは耐えられません。こんな会社でも３年間勤めないと転職市場では我慢できない若者の烙印を押されてしまうのでしょうか。プア子からの辛口アドバイスA：見られるのはあなたの活躍。どうも。年収200万円台のプア子です。昨日は奮発してマックでセットを買っちゃったの。お悩み見たわ。ひとつの会社に三年勤めないと書類選考でハンデを背負うのは確か。でも、ただ三年勤めれば有利なこともないのよ。大切なのは、何を考え、どんな結果を出してきたか。今の職場は確かにひどい環境だと思うけど、多かれ少なかれ、どこの会社にも似たような状態はあるもの。転職したい理由を環境のせいにせず、成果が正当に評価されない、くらいの気概がほしいわ。不動産って売れば売るほど稼げるんでしょ。うらやましいわよ。今回の説教環境のせいにせず、結果で勝負しなさい。情報提供元：@typelivedoor求人転職livedoor求人転職は、あなたがどんな求人情報を探しているのかを瞬時に判断しておすすめするコンテンツ。心機一転、新たな環境で活躍できる！積極採用を行っている企業特集など、耳寄り転職情報を提供中です。', 'フラワープリントvs.マリンスタイルあなたはどっちがお好き？3月に入り、そろそろ気になるのが春夏のファッショントレンド。6日に開催された東京ガールズコレクション10SS以下、TGCでは、全体的にロンパース、オールインワンと呼ばれるリラックスモードのつなぎスタイルやフラワープリントが多く見られました。また、昨年から引き続きボーダーを多用したマリンスタイルも人気を集めているようです。TGCは日本のリアルクローズを世界へ向けてアピールすることを目的に初開催された国内最大級のファッションフェスタ。今回で記念すべき10回目を迎え、国内外からさらに多くの注目を集めています。香里奈さん着用のCECILMcBEEのフラワー柄オールインワンは、ゴールドのビッグブレスレットで大人っぽさを演出しているのがポイント。峰えりかさん着用のマキシ丈ワンピースはタウンユースもリゾート使いも両方出来そうですね。クリスティーナさんがラブリーに着こなした、31Sonsdemodeのワンピースは勝負デートの時に大活躍しちゃうかも。藤井リナさん着用のボーダーのカットソーは、ショートパンツに合わせても、流行のボーイフレンドデニムとラフに着るのも可愛い優秀アイテム。トリンドル玲奈さんのスウィートなコーディネートは、着ているだけでHAPPYになれそうですね。フラワーとボーダーという、不変的なモチーフだからこそ、ボトムや小物で変化をつけて、オンオフ両方着回すのが賢いお仕事ガール。皆さんはどのコーディネートがお好みですか？東京ガールズコレクション', 'あなたの打たれ強さ度は?イイ女を作る朝型生活などライフスタイル週間ランキング人間関係をスムーズにするヒントやライフハック、節約ネタなど、今すぐ役立つ情報が詰まったPeachyのライフスタイルカテゴリ。このカテゴリのなかから、2012年6月27日〜7月4日の間に最も多く読まれた記事TOP5をご紹介します！第1位：1年間洗顔せずお化粧を続けるとこうなる!?365日分のお化粧を施したらドロドロになってしもうたよあなたはこんなことを考えたことがありますか？私って、1年にどれくらい化粧品を使っているんだろう？そんな素朴な疑問を、モデルを使って実際に目に見える形にしてしまった、2人のオランダ人アーティストがいました。彼らの名は、Lernert&Sanderふたりとも男性です！。ただしこのふたりが表現したかったことは、人は1年に化粧品をどれくらい使うのかではなく、どれくらい化粧品を使えば、人は自然な状態からとんでもない状態になるのかということ。彼らはそうだ！化粧品を1年分塗り重ねていけば、きっととんでもなくなるはずだ！と、考えたわけです。第2位：あなたの打たれ強さ度を診断恋がうまくいかなかったり、理不尽なことで上司に怒られたり。生きていると凹むことっていっぱいありますよね。どん底気分のとき、あなたはすぐに回復できる人？それともなかなか立ち直れないほう？恋愛カウンセラーゆまさん監修の究極の恋愛科学では、あなたの打たれ強さ度をチェックできる心理テストを公開しています。第3位：レパートリーに悩むママ必見！夏のカンタン朝ごはんとは？あなたは毎日朝ごはんを食べていますか？何かと慌しい朝。時間がなくて毎日同じメニューなんて方も多いのではないでしょうか。ひとりだと適当にパンやおにぎりをコンビニで購入したり、ダイエットのために朝食を抜くこともあるかもしれませんが、家庭を持っているとそうはいきませんよね。第4位：いい女は朝を制す！朝型女子7の習慣おはようございますという挨拶からも気品が漂ってきそうな朝からシャキっと美しい女性、あなたの周りにいませんか？今回はそんな女性たちが実践しているであろう、いい女に欠かせない朝の習慣をまとめてみました。第5位：母性本能が強い女性6の特徴弱ってる男性を前にこの人は私がいないとダメだ、何とかしなきゃ！とつい使命感に燃えてしまうことありませんか？今回はそんな母性本能が強い女性の特徴をまとめてみました。以上、先週のライフスタイルカテゴリ週間ランキングでした！', 'スマホでゲリラ豪雨に備えるデジ通暑い夏が戻り、同時に、連日のようにゲリラ豪雨が都心を襲っている。無防備なまま出くわすと、濡れるだけではない被害に遭う。スマートフォン高機能携帯電話を使って、少しでもゲリラ豪雨を避けよう。昨年より３割も多い！東京アメッシュ大島克彦＠katsuoshdigi2デジ通digi2はデジタル通の略です。現在のデジタル機器は使いこなしが難しくなっています。皆さんがデジタル機器の通に近づくための情報を、皆さんよりすこし通な執筆陣が提供します。関連記事スマホで熱中症を避けるデジ通iPhoneに差すだけ、はんだづけ不用、3500円の空間線量計自作キットガイガーカウンター特集：商品紹介読書の夏！洋書好きにKindleがオススメな理由デジ通真夏のオススメホラー映画BEST5デジ通夏休み特集読書感想文にスマホを活用する', '音声認識で英語の発音練習しようSiriを使うiPhoneアプリReal英会話デジ通アップルのSiriは音声認識機能を持つバーチャルアシスタントだ。この音声認識機能を使用して、英会話の発音練習に活用している方もいると思われるが、英会話学習アプリでもこの機能を活用し始めている。その1つが、LTBoxのReal英会話だ。Real英会話はiPhoneやiPadなどに対応したユニバーサルアプリで、以前から実生活で使われるようなリアルな英会話の学習ができた人気のアプリだ。2012年5月に行われたアップデートバージョン3.1では、Siriによる音声認識を使った発音練習機能が追加されている。Real英会話自体は、TOEICなどの練習よりも、主に日常生活で使われているような英会話のリスニングなどを練習できるアプリで、それ自体にも価値があるが、発音認識機能がついたことでさらに価値を増している。単語や文法などは独学で学習できたとしても、発音練習を独学でやるのは難しい。最近利用が本格化してきた音声認識機能を活用すれば、独学での発音練習も現実的になってきた。Siriの音声認識機能をそのまま利用する実際に試してみると、アプリで学習する日常会話をSiriによる音声認識機能を使ってそのまま学習できるようになっている。アプリのフレーズクイズ内に発音練習があり、ここで練習できる。発音練習画面では、日本語の意味と、英語が表示される。ここに表示される英語を発音し音声認識で正しく認識できれば、そのフレーズを学習したことになる。認識精度はSiriの音声認識機能をそのまま使用するため非常に高い。しかし、日本の多くの学習者は日本語アクセントの英語で発音することが多いため、認識という点ではかなり厳しいのではないかと思われる。Siriの音声認識機能は、英語でも、アメリカ、イギリス、オーストラリアの3つの地域別に設定できる。日本語アクセントの英語という設定はないので、Real英会話アプリで機械的に緩く認識するようなことはない。日本語アクセントの英語で認識させようと思っても、ネイティブが聞き間違えるように間違えて認識されてしまう。Real英会話での学習のコツとしては、発音例も画面をタップすればすぐに再生されるので、英文を読むのではなく、発音例をまねてそのまま発音することだ。筆者も日本語アクセントの英語がやっとだが、なかなか認識されないフレーズについては、なるべくそっくりにまねして発音することで何とか認識されることが多かった。発音がうまくない多くの日本人にとって、かなり厳しい認識精度だが、発音練習の1つとして活用するにはもってこいのアプリだろう。Real英会話上倉賢@kamikuradigi2デジ通digi2はデジタル通の略です。現在のデジタル機器は使いこなしが難しくなっています。皆さんがデジタル機器の通に近づくための情報を、皆さんよりすこし通な執筆陣が提供します。デジ通の記事をもっと見るiPhone4Sの音声認識機能は使える新しいiPadでも使える音声認識英語の音声認識で英会話練習自分の発音を客観的に知る方法サービス終了後はどうなる！電子書籍は永遠に読めるのか\\tキンドルが日本に参入する？Amazonの電子書籍をおさらいiPhoneが据え置きゲーム機も殺す？多機能化するスマートフォンキヤノン純正インクキャノンインクタンク5色マルチパックBCI3213205MPキヤノン販売元：Amazon.co.jpクチコミを見る']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4eN-vSeS1NB",
        "colab_type": "text"
      },
      "source": [
        "- BoW + TFIDFでベクトル化する\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oob3Q6d_reQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 200記事をチョイス\n",
        "choosed_docs = cleaned_docs[:500]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rusv-M5_n2lO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_token(text):\n",
        "    t = Tokenizer()\n",
        "    tokens = t.tokenize(text)\n",
        "    word = \"\"\n",
        "    for token in tokens:\n",
        "        part_of_speech = token.part_of_speech.split(\",\")[0]\n",
        "        if part_of_speech == \"名詞\":\n",
        "            word +=token.surface + \" \"\n",
        "        if part_of_speech == \"動詞\":\n",
        "            word +=token.base_form+ \" \"\n",
        "        if part_of_speech == \"形容詞\":\n",
        "            word +=token.base_form+ \" \"\n",
        "        if part_of_speech == \"形容動詞\":\n",
        "            word +=token.base_form+ \" \"\n",
        "    return word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKptt4nDle9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = np.array([])\n",
        "\n",
        "for doc in choosed_docs:\n",
        "    token = get_token(doc)\n",
        "    tokens = np.append(tokens, token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y_kqt5NqupK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_vectorizer = CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
        "tf = c_vectorizer.fit_transform(tokens)\n",
        "\n",
        "t_transformer = TfidfTransformer()\n",
        "tfidf = t_transformer.fit_transform(tf).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcn-w3HRqd53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_cosine_similarity(v1, v2):\n",
        "    \"\"\"\n",
        "    ベクトルv1, v2のcos類似度の算出\n",
        "    \"\"\"\n",
        "    return sum([a*b for a, b in zip(v1, v2)])/(sum(map(lambda x: x*x, v1))**0.5 * sum(map(lambda x: x*x, v2))**0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYuF79-r2Bs6",
        "colab_type": "text"
      },
      "source": [
        "- あるニュースに一番cos類似度が近いニュースを出力する関数の作成\n",
        "- 別の類似度手法を1つ調べて関数に組み込む(切り替えられるようにする)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNcHROTqQ16F",
        "colab_type": "code",
        "outputId": "94b89a54-5e52-4a38-dcdd-40e5cd7e06ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# 最初の記事\n",
        "v1 = tfidf[0]\n",
        "sims = np.array([])\n",
        "\n",
        "for index in range(len(tfidf)):\n",
        "    v2 = tfidf[index]\n",
        "    \n",
        "    if np.allclose(v1, v2) == False:\n",
        "        sims = np.append(sims, calc_cosine_similarity(v1, v2))\n",
        "\n",
        "highest_similarity = np.argmax(sims)\n",
        "print(sims[highest_similarity])\n",
        "print(highest_similarity)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0923204502645693\n",
            "271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-steqCzzKI9",
        "colab_type": "code",
        "outputId": "9d794a48-48f9-4641-c82e-f71b86b4383e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "print(choosed_docs[0])\n",
        "print(tokens[0])\n",
        "print(choosed_docs[highest_similarity])\n",
        "print(tokens[highest_similarity])\n",
        "print(re.findall(url_reg, documents[0]))\n",
        "print(re.findall(url_reg, documents[highest_similarity]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NY名物イベントが日本でも！名店グルメを気軽に楽しむニューヨークで20年続いている食の祭典レストランウィーク。その日本版がダイナーズクラブ特別協賛のもと7月30日よりスタート。8月31日までの期間中、青山六本木、丸の内、銀座、横浜のエリアから、ラグジュアリーレストラン81店舗がこのイベントのために特別用意したランチメニュー2010円税サ別、ディナー5000円税サ別を気軽に楽しめる、とっておきのイベントです。実行委員長には、学校法人服部学園、服部栄養専門学校理事長校長であり医学博士でもある服部幸應氏を迎え、実行委員に石田純一さん、LABETTOLAオーナーシェフ落合務氏、フードアナリスト協会会長、高賀右近氏、つきぢ田村三代目、田村隆氏に、そして放送作家脚本家の小山薫堂さんなど、食のスペシャリストたちが勢揃い。参加レストランには、ミシュランのフランス版、東京版ともに星を獲得している吉野建シェフのレストランタテルヨシノ汐留や、日本料理の名門つきぢ田村、金田中庵、赤坂璃宮にmikuniMARUNOUCHIなど、日本を代表するレストランがずらり。イベント期間の〜8月19日までは、特別協賛のダイナーズクラブカード会員、またはシティバンクに口座を持つシティゴールドメンバーが楽しめる先行期間となりますが、その後は誰でも参加できるので、日程のチェックは必須。予約方法は必ず事前に、各店舗に問合せを行いジャパンレストランウィーク2010での予約であることを伝えればOK！憧れていたレストランの料理をリーズナブルにいただけるチャンスです！極上の味とラグジュアリーな空間を満喫。そんな幸せを実感できるジャパンレストランウィーク2010にぜひ参加しててみてはいかがですか？JAPANRESTAURANTWEEK2010公式サイト\n",
            "NY 名物 イベント 日本 名店 グルメ 気軽 楽しむ ニューヨーク 20 年 続く いる 食 祭典 レストラン ウィーク 日本 版 ダイナーズクラブ 特別 協賛 もと 7 月 30 日 スタート 8 月 31 日 期間 中 青山 六本木 丸の内 銀座 横浜 エリア ラグジュアリーレストラン 81 店舗 イベント ため 特別 用意 する ランチ メニュー 2010 円 税 サ 別 ディナー 5000 円 税 サ 別 気軽 楽しめる とる おく イベント 実行 委員 長 学校 法人 服部 学園 服部 栄養 専門 学校 理事 長 校長 医学 博士 服部 幸 應氏 迎える 実行 委員 石田 純一 さん LABETTOLA オーナー シェフ 落合 務 氏 フード アナリスト 協会 会長 高賀 右近 氏 つく ぢ 田村 三 代目 田村 隆 氏 放送 作家 脚本 家 小山 薫 堂 さん 食 スペシャリスト たち 勢揃い 参加 レストラン ミシュラン フランス 版 東京 版 とも 星 獲得 する いる 吉野 建 シェフ レストランタテルヨシノ 汐留 日本 料理 名門 つき ぢ 田村 金 田中 庵 赤坂 璃宮 mikuniMARUNOUCHI 日本 代表 する レストラン イベント 期間 8 月 19 日 特別 協賛 ダイナーズクラブカード 会員 シティバンク 口座 持つ シティゴールドメンバー 楽しめる 先行 期間 なる その後 誰 参加 できる 日程 チェック 必須 予約 方法 事前 店舗 問合せ 行う ジャパンレストランウィーク 2010 予約 こと 伝える OK 憧れる いる レストラン 料理 リーズナブル いただける チャンス 極上 味 ラグジュアリー 空間 満喫 幸せ 実感 できる ジャパンレストランウィーク 2010 参加 する てる みる いかが JAPANRESTAURANTWEEK 2010 公式 サイト \n",
            "話題気を付けて！道や駅で増えるケータイ事故街中で携帯電話やスマートフォンを利用する人が増えている。自分の手元の画面ばかり気になり周りが見えていない人が多いのか、人とぶつかったり、駅のホームから落下する事故が増えているという。今一度気を付ける必要があるだろう。ＮＨＫが１０月６日にクローズアップ現代で取り上げたのが、このケータイ事故だ。ケータイを見ながら駅のホームを歩いていてそのまま落ちるという事故は全国各地で増えている。だが、もっとも危険を感じているのが視覚障碍者だという。ケータイを歩きながら利用している人にありがちなのが突然止まるという行為だろう。メールを受信したり、細かい文章を打つときに立ち止まる人が多いのだ。視覚障害を持っている人がこういった人たちにぶつかったり、彼らをよけようとしてホームから転落することがあるのだという。スマートフォンの普及に伴い、携帯電話で可能なことが増えた。従来のメールや電話機能に加え、位置情報の確認やゲーム、ツイッターなどのソーシャルネットワークサービス、そしてワンセグでのテレビ鑑賞など多くのことが可能になった。一日の中でケータイやスマホに向かっている時間が増えている人も多いだろう。周りの人の迷惑になっていないか、今一度確認すべきだ。自分が事故を引き起こす立場にならないように気を付けよう。2011年10月6日放送ケータイ事故駅のホームでいま何がNHKクローズアップ現代関連記事話題電気ポット？電気ケトル？節電に効果的な湯沸かし家電とはサマンサタバサがキャノンとコラボ！限定デジカメIXY600Fシャンパンピンク発売話題アンドロイドアプリを徹底解説！Appmaxがデザインを一新オリオン電機から発売！USB接続HDDへ録画対応液晶テレビ話題まずはソフトバンクが優勢？iPhone4Sいよいよ発売\n",
            "話題 気 付ける 道 駅 増える ケータイ 事故 街 中 携帯 電話 スマート フォン 利用 する 人 増える いる 自分 手元 画面 気 なる 周り 見える いる 人 多い の 人 ぶつかる 駅 ホーム 落下 する 事故 増える いる いう 今 一 度 気 付ける 必要 ある ＮＨＫ １０月 ６ 日 クローズアップ 現代 取り上げる の ケータイ 事故 ケータイ 見る 駅 ホーム 歩く いる 落ちる 事故 全国 各地 増える いる 危険 感じる いる の 視覚 障碍 者 いう ケータイ 歩く 利用 する いる 人 ある がち の 止まる 行為 メール 受信 する 細かい 文章 打つ とき 立ち止まる 人 多い の 視覚 障害 持つ いる 人 いう 人 たち ぶつかる 彼ら よう ホーム 転落 する こと ある の いう スマート フォン 普及 伴う 携帯 電話 可能 こと 増える 従来 メール 電話 機能 加える 位置 情報 確認 ゲーム ツイッター ソーシャルネットワークサービス ワンセグ テレビ 鑑賞 多く こと 可能 なる 一 日 中 ケータイ スマホ 向かう いる 時間 増える いる 人 多い 周り 人 迷惑 なる いる 今 一 度 確認 する 自分 事故 引き起こす 立場 なる よう 気 付ける 2011 年 10 月 6 日 放送 ケータイ 事故 駅 ホーム いま 何 NHK クローズアップ 現代 関連 記事 話題 電気 ポット 電気 ケトル 節電 効果 的 湯沸かし 家電 サマンサタバサ キャノン コラボ 限定 デジカメ IXY 600 F シャンパン ピンク 発売 話題 アンドロイドアプリ 徹底 解説 Appmax デザイン 一新 オリオン電機 発売 USB 接続 HDD 録画 対応 液晶 テレビ 話題 ソフトバンク 優勢 iPhone 4 S 発売 \n",
            "['http://news.livedoor.com/article/detail/4931238/']\n",
            "['http://news.livedoor.com/article/detail/5929932/']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V0WhwBJh4eF",
        "colab_type": "text"
      },
      "source": [
        "- なぜそのような結果になったのか考察する  \n",
        "記事の内容的には全く類似性は見れれなかったため、名詞などよりも「する」「いる」などの動詞の頻出具合で類似度が高いと判断された？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s7JtX0vFmbd",
        "colab_type": "text"
      },
      "source": [
        "## 【問題6】感情分析\n",
        "https://www.tensorflow.org/tutorials/keras/basic_text_classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH75yB7o62bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMDBをカレントフォルダにダウンロード\n",
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "# 解凍\n",
        "!tar zxf aclImdb_v1.tar.gz\n",
        "# aclImdb/train/unsupはラベル無しのため削除\n",
        "!rm -rf aclImdb/train/unsup\n",
        "# IMDBデータセットの説明を表示\n",
        "#!cat aclImdb/README"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi1wNsdZleR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
        "train_text, train_y = train_review.data, train_review.target\n",
        "\n",
        "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
        "test_text, test_y = test_review.data, test_review.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqUQ42GHlqOz",
        "colab_type": "code",
        "outputId": "07a33a16-a7d7-4962-96d5-7ca031627923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "train_text[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\",\n",
              " 'Words can\\'t describe how bad this movie is. I can\\'t explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clichés, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won\\'t list them here, but just mention the coloring of the plane. They didn\\'t even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys\\' side all the time in the movie, because the good guys were so stupid. \"Executive Decision\" should without a doubt be you\\'re choice over this one, even the \"Turbulence\"-movies are better. In fact, every other movie in the world is better than this one.',\n",
              " 'Everyone plays their part pretty well in this \"little nice movie\". Belushi gets the chance to live part of his life differently, but ends up realizing that what he had was going to be just as good or maybe even better. The movie shows us that we ought to take advantage of the opportunities we have, not the ones we do not or cannot have. If U can get this movie on video for around $10, it´d be an investment!',\n",
              " 'There are a lot of highly talented filmmakers/actors in Germany now. None of them are associated with this \"movie\".<br /><br />Why in the world do producers actually invest money in something like this this? You could have made 10 good films with the budget of this garbage! It\\'s not entertaining to have seven grown men running around as dwarfs, pretending to be funny. What IS funny though is that the film\\'s producer (who happens to be the oldest guy of the bunch) is playing the YOUNGEST dwarf.<br /><br />The film is filled with moments that scream for captions saying \"You\\'re supposed to laugh now!\". It\\'s hard to believe that this crap\\'s supposed to be a comedy.<br /><br />Many people actually stood up and left the cinema 30 minutes into the movie. I should have done the same instead of wasting my time...<br /><br />Pain!',\n",
              " 'I\\'ve just had the evidence that confirmed my suspicions. A bunch of kids, 14 to 22 put on the DVD of \"Titanic\" on a fantastic state of the art mega screen home entertainment type deal. Only two of them had actually seen it before. But they all had seen the moment of Kate, Leo and Celine Dion so many times that most of them felt they had seen the whole movie. Shortly after the epic started, they started to get restless, some of them left asking the others -- \"call us when the iceberg appears\" Over an hour and a half into the movie, only the two girls who had seen the movie before, were still there. They started shouting: iceberg, iceberg. A stampede followed, they all came back to see the sinking of the Titanic. They sat open mouthed, emitting Ohs and far outs. So, just like I thought when the movie first burst into the scene. What is this? One and a half hours waiting for the bloody thing to sink but what about the rest of the of it. Dr. Zivagho, for instance, had a similar running time, but think how much takes place in that film within the same period of time. In \"Titanic\" Leo teaches Kate how to spit. Look at the faces and hands of the, supposedly, creme de la creme in the first class dining room of the ship. Look at the historical details, if you can find them. The storyline is so thin that they have to introduce guns and shootings in a ship that is about to sink. The real sinking here is of film standards. All the efforts are focus on special effects and opening week ends. The film went on to become the highest grossing movie of all time so, what do I know?',\n",
              " \"The Movie was sub-par, but this Television Pilot delivers a great springboard into what has become a Sci-Fi fans Ideal program. The Actors deliver and the special effects (for a television series) are spectacular. Having an intelligent interesting script doesn't hurt either.<br /><br />Stargate SG1 is currently one of my favorite programs.\",\n",
              " \"This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending.\",\n",
              " \"the single worst film i've ever seen in a theater. i saw this film at the austin film festival in 2004, and it blew my mind that this film was accepted to a festival. it was an interesting premise, and seemed like it could go somewhere, but just fell apart every time it tried to do anything. first of all, if you're going to do a musical, find someone with musical talent. the music consisted of cheesy piano playing that sounded like they were playing it on a stereo in the room they were filming. the lyrics were terribly written, and when they weren't obvious rhymes, they were groan-inducing rhymes that showed how far they were stretching to try to make this movie work. and you'd think you'd find people who could sing when making a musical, right? not in this case. luckily they were half talking/half singing in rhyme most of the time, but when they did sing it made me cringe. especially when they attempted to sing in harmony. and that just addresses the music. some of the acting was pretty good, but a lot of the dialog was terrible, as well as most of the scenes. they obviously didn't have enough coverage on the scenes, or they just had a bad editor, because they consistently jumped the line and used terrible choices while cutting the film. at least the director was willing to admit that no one wanted the script until they added the hook of making it a musical. i hope the investors make sure someone can write music before making the same mistake again.\",\n",
              " \"The plot of this terrible film is so convoluted I've put the spoiler warning up because I'm unsure if I'm giving anything away. The audience first sees some man in Jack the Ripper garb murder an old man in an alley a hundred years ago. Then we're up to modern day and a young Australian couple is looking for a house. We're given an unbelievably long tour of this house and the husband sees a figure in an old mirror. Some 105 year old woman lived there. There are also large iron panels covering a wall in the den. An old fashioned straight-razor falls out when they're renovating and the husband keeps it. I guess he becomes possessed by the razor because he starts having weird dreams. Oh yeah, the couple is unable to have a baby because the husband is firing blanks. <br /><br />Some mold seems to be climbing up the wall after the couple removes the iron panels and the mold has the shape of a person. Late in the story there is a plot about a large cache of money & the husband murders the body guard & a co-worker and steals the money. His wife is suddenly pregnant. <br /><br />What the hell is going on?? Who knows?? NOTHING is explained. Was the 105 year old woman the child of the serial killer? The baby sister? WHY were iron panels put on the wall? How would that keep the serial killer contained in the cellar? Was he locked down there by his family & starved to death or just concealed? WHO is Mr. Hobbs and why is he so desperate to get the iron panels?? He's never seen again. WHY was the serial killer killing people? We only see the one old man murdered. Was there a pattern or motive or something?? WHY does the wife suddenly become pregnant? Is it the demon spawn of the serial killer? Has he managed to infiltrate the husband's semen? And why, if the husband was able to subdue and murder a huge, burly security guard, is he unable to overpower his wife? And just how powerful is the voltage system in Australia that it would knock him across the room simply cutting a light wire? And why does the wife stay in the house? Is she now possessed by the serial killer? Is the baby going to be the killer reincarnated? <br /><br />This movie was such a frustrating experience I wanted to call my PBS station and ask for my money back! The ONLY enjoyable aspect of this story was seeing the husband running around in just his boxer shorts for a lot of the time, but even that couldn't redeem this muddled, incoherent mess.\",\n",
              " 'I had no idea that Mr. Izzard was so damn funny, It really boggles the mind that he is not more well known! His command over the crowd and his timing is perfect.The monologue about Star Wars will kill ya too! If only all the stand up performers had his wit...']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTHuiItrncD9",
        "colab_type": "text"
      },
      "source": [
        "前処理は下記を実施  \n",
        "- 記号・htmlタグ・lfの削除\n",
        "- lemmatize\n",
        "- 小文字で統一\n",
        "- text → sequenceに変換\n",
        "- 最大長のsequenceに併せてパディング"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-X6jrJZDm-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(reg_str,'',text, re.UNICODE)\n",
        "    text = text.lower()\n",
        "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
        "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
        "    text = [word for word in text if not word in stop_words]\n",
        "    text = \" \".join(text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0mmId5dDoR6",
        "colab_type": "code",
        "outputId": "a9647a51-6d1d-4f9a-b382-4a09a14db68a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "max_features = 6000\n",
        "\n",
        "# 正規表現一覧\n",
        "symbol_reg = r'[^\\w\\s]'\n",
        "html_reg = r'<.*?>'\n",
        "lf_reg = r'\\\\\\'[a-z]'\n",
        "reg_str = f'{symbol_reg}|{html_reg}|{lf_reg}'\n",
        "reg_str = re.compile(reg_str)\n",
        "\n",
        "total_reviews = train_text + test_text\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "for index in range(len(total_reviews)):\n",
        "    total_reviews[index] = clean_text(total_reviews[index])\n",
        "\n",
        "    \n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(total_reviews)\n",
        "\n",
        "\n",
        "for index in range(len(train_text)):\n",
        "    train_text[index] = clean_text(train_text[index])\n",
        "\n",
        "    \n",
        "for index in range(len(test_text)):\n",
        "    test_text[index] = clean_text(test_text[index])\n",
        "\n",
        "\n",
        "train_tokens = tokenizer.texts_to_sequences(train_text)\n",
        "test_tokens = tokenizer.texts_to_sequences(test_text)\n",
        "\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in total_reviews])\n",
        "train_x = pad_sequences(train_tokens, maxlen=max_length, padding='post')\n",
        "test_x = pad_sequences(test_tokens, maxlen=max_length, padding='post')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSOVsxWkfFcW",
        "colab_type": "code",
        "outputId": "71c57058-8820-4f41-95cc-8bfdc0d0a1ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "# set parameters:\n",
        "batch_size = 100\n",
        "embedding_dims = 128\n",
        "epochs = 100\n",
        "\n",
        "\n",
        "print('Build model...')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features,\n",
        "                    embedding_dims,\n",
        "                    input_length=max_length))\n",
        "model.add(LSTM(64, return_sequences = True))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(20, activation=\"relu\"))\n",
        "model.add(Dropout(0.05))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "print('Train...')\n",
        "\n",
        "es_cb = EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto')\n",
        "cp_cb = ModelCheckpoint(filepath = 'imdb_{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
        "                                            patience=3, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)\n",
        "\n",
        "model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, validation_data=(test_x, test_y), callbacks=[es_cb,cp_cb, learning_rate_reduction])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Train...\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/100\n",
            "25000/25000 [==============================] - 969s 39ms/step - loss: 0.4779 - acc: 0.7725 - val_loss: 0.3250 - val_acc: 0.8612\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.32503, saving model to imdb_01-0.33.hdf5\n",
            "Epoch 2/100\n",
            "25000/25000 [==============================] - 966s 39ms/step - loss: 0.2561 - acc: 0.9014 - val_loss: 0.3029 - val_acc: 0.8717\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.32503 to 0.30285, saving model to imdb_02-0.30.hdf5\n",
            "Epoch 3/100\n",
            "25000/25000 [==============================] - 968s 39ms/step - loss: 0.1925 - acc: 0.9299 - val_loss: 0.3283 - val_acc: 0.8690\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.30285\n",
            "Epoch 4/100\n",
            "25000/25000 [==============================] - 963s 39ms/step - loss: 0.1511 - acc: 0.9455 - val_loss: 0.3411 - val_acc: 0.8633\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.30285\n",
            "Epoch 00004: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5cb00a11d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agY7jERpS7qO",
        "colab_type": "text"
      },
      "source": [
        "90%超えない、、  \n",
        "モデルを変えて試してみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKhdzh62mn7v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "12662fd9-bd58-4031-e3e0-0fec781cc0d0"
      },
      "source": [
        "# set parameters::\n",
        "batch_size = 100\n",
        "embedding_dims = 128\n",
        "epochs = 100\n",
        "layers = 2\n",
        "learning_rate=1e-3\n",
        "units=64\n",
        "dropout_rate=0.2\n",
        "\n",
        "print('Build model...')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features,\n",
        "                    embedding_dims,\n",
        "                    input_length=max_length))\n",
        "model.add(Flatten())\n",
        "\n",
        "for _ in range(layers-1):\n",
        "    model.add(Dense(units=units, activation='relu'))\n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "optimizer = optimizers.Adam(lr=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "print('Train...')\n",
        "\n",
        "es_cb = EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto')\n",
        "cp_cb = ModelCheckpoint(filepath = 'imdb_{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
        "                                            patience=3, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)\n",
        "model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, validation_data=(test_x, test_y), verbose=2, callbacks=[es_cb,cp_cb, learning_rate_reduction])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Train...\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/100\n",
            " - 12s - loss: 0.5124 - acc: 0.7640 - val_loss: 0.3135 - val_acc: 0.8680\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.31349, saving model to imdb_01-0.31.hdf5\n",
            "Epoch 2/100\n",
            " - 11s - loss: 0.2206 - acc: 0.9179 - val_loss: 0.3214 - val_acc: 0.8638\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.31349\n",
            "Epoch 3/100\n",
            " - 11s - loss: 0.0794 - acc: 0.9786 - val_loss: 0.3728 - val_acc: 0.8614\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.31349\n",
            "Epoch 00003: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5c7016fa58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYc5n6157vU7",
        "colab_type": "text"
      },
      "source": [
        "あまり変わらない、、  \n",
        "次は前処理をtfidfに変えてみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg95DoeKBO0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
        "train_text, train_y = train_review.data, train_review.target\n",
        "\n",
        "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
        "test_text, test_y = test_review.data, test_review.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZtrSPIBSzm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
        "    NGRAM_RANGE = (1, 2)\n",
        "    TOP_K = 20000\n",
        "    TOKEN_MODE = 'word'\n",
        "    MIN_DOC_FREQ = 2\n",
        "    kwargs = {\n",
        "        'ngram_range' : NGRAM_RANGE,\n",
        "        'dtype' : 'int32',\n",
        "        'strip_accents' : 'unicode',\n",
        "        'decode_error' : 'replace',\n",
        "        'analyzer' : TOKEN_MODE,\n",
        "        'min_df' : MIN_DOC_FREQ,\n",
        "    }\n",
        "    \n",
        "    # Learn Vocab from train texts and vectorize train and val sets\n",
        "    tfidf_vectorizer = TfidfVectorizer(**kwargs)\n",
        "    x_train = tfidf_vectorizer.fit_transform(train_texts)\n",
        "    x_val = tfidf_vectorizer.transform(val_texts)\n",
        "    \n",
        "    # Select best k features, with feature importance measured by f_classif\n",
        "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
        "    selector.fit(x_train, train_labels)\n",
        "    x_train = selector.transform(x_train).astype('float32')\n",
        "    x_val = selector.transform(x_val).astype('float32')\n",
        "    return x_train, x_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCVFwP-58Ip7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "7a848b32-cc95-41a1-918b-0899dc4400bb"
      },
      "source": [
        "layers = 2\n",
        "learning_rate=1e-3\n",
        "epochs=100\n",
        "batch_size=128\n",
        "units=64\n",
        "dropout_rate=0.2\n",
        "\n",
        "\n",
        "# Vectorize the data\n",
        "x_train, x_val = ngram_vectorize(train_text, train_y, test_text)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Dropout(rate=dropout_rate, input_shape=x_train.shape[1:]))\n",
        "\n",
        "for _ in range(layers-1):\n",
        "    model.add(Dense(units=units, activation='relu'))\n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "optimizer = optimizers.Adam(lr=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Create callback for early stopping on validation loss. If the loss does\n",
        "# not decrease on two consecutive tries, stop training\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
        "\n",
        "\n",
        "print('Train...')\n",
        "# Train and validate model\n",
        "history = model.fit(x_train, train_y, epochs=epochs, validation_data=(x_val, test_y),\n",
        "                    verbose=2, batch_size=batch_size, callbacks=callbacks)\n",
        "\n",
        "# Print results\n",
        "history = history.history\n",
        "val_acc = history['val_acc'][-1]\n",
        "val_loss = history['val_loss'][-1]\n",
        "print ('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "        acc=val_acc, loss=val_loss))\n",
        "\n",
        "# Save model\n",
        "model.save('IMDB_mlp_model_' + str(val_acc) + '_binary_crossentropy'+ '.h5')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:1616: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/100\n",
            " - 4s - loss: 0.4731 - acc: 0.8601 - val_loss: 0.3263 - val_acc: 0.8864\n",
            "Epoch 2/100\n",
            " - 3s - loss: 0.2348 - acc: 0.9208 - val_loss: 0.2519 - val_acc: 0.9016\n",
            "Epoch 3/100\n",
            " - 3s - loss: 0.1692 - acc: 0.9406 - val_loss: 0.2337 - val_acc: 0.9061\n",
            "Epoch 4/100\n",
            " - 3s - loss: 0.1351 - acc: 0.9529 - val_loss: 0.2300 - val_acc: 0.9050\n",
            "Epoch 5/100\n",
            " - 3s - loss: 0.1103 - acc: 0.9632 - val_loss: 0.2332 - val_acc: 0.9041\n",
            "Epoch 6/100\n",
            " - 3s - loss: 0.0953 - acc: 0.9686 - val_loss: 0.2404 - val_acc: 0.9020\n",
            "Validation accuracy: 0.90196, loss: 0.240361744556427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ-bYU9Hilxp",
        "colab_type": "text"
      },
      "source": [
        "90を超えた...  \n",
        "lstmに変えればさらに気持ち上がるかも？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDN-MiibFszE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## 【問題8】自然言語処理の応用事例\n",
        "http://www.textmining.jp/case_keyword.html  \n",
        "http://www.textmining.jp/game_platform.html"
      ]
    }
  ]
}